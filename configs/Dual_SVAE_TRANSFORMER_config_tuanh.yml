model_dir: training_ckpts/Dual_SVAE_Transformer

Architecture: Transformer

#false if LSTM
daisy_chain_variables: True

training_label_file: /export/home/tnguyen/datasets/WMT19-tritexts/train/BPE/wmt_clean.en-fr-de.BPE.fr
training_feature_file: /export/home/tnguyen/datasets/WMT19-tritexts/train/BPE/wmt_clean.en-fr-de.BPE.de
eval_feature_file: /export/home/tnguyen/datasets/WMT19-tritexts/eval/BPE/euelections_dev2019.de-fr.BPE.fr
eval_label_file: /export/home/tnguyen/datasets/WMT19-tritexts/eval/BPE/euelections_dev2019.de-fr.BPE.de
src_vocab_path: /export/home/tnguyen/datasets/WMT19-tritexts/vocab/base/vocab.BPE.fr
tgt_vocab_path: /export/home/tnguyen/datasets/WMT19-tritexts/vocab/base/vocab.BPE.de

optimizer_parameters:
        optimizer: LazyAdamOptimizer #GradientDescentOptimizer
        learning_rate: 1.0 # The scale constant.
        decay_type: noam_decay_v2
        decay_params:
            model_dim: 512
            warmup_steps: 4000
        decay_step_duration: 8
        start_decay_steps: 0
        clip_gradients: Null
        gradients_accum: 1
        
#OPTIMIZER_CLS_NAMES = {
#    "Adagrad": train.AdagradOptimizer,
#    "Adam": train.AdamOptimizer,
#    "Ftrl": train.FtrlOptimizer,
#    "Momentum": lambda learning_rate: train.MomentumOptimizer(learning_rate, momentum=0.9),  # pylint: disable=line-too-long
#    "RMSProp": train.RMSPropOptimizer,
#    "SGD": train.GradientDescentOptimizer,
#    }

mode: Training

verage_loss_in_time: true
label_smoothing: 0.1
beam_width: 5
length_penalty: 0.6

num_devices: 3
num_threads: 20

iteration_number: 300000

training_batch_type: examples

example_sampling_distribution: Natural

dataprocess_version: 

training_batch_size: 16

eval_batch_size: 16

max_len: 80

Standard: true
position_mask: false
Fusion_layer: true
generic_batch: true
projector_masking: false
src_masking: true
tgt_masking: true
Generic_region_adversarial_training: false
src_adv_training: false
tgt_adv_training: false
embedding_adv_training: false
encoder_adv_training: false

discriminator_optimizer_parameters:
        optimizer: LazyAdamOptimizer
        learning_rate: 0.002 # The scale constant.                
        decay_type: noam_decay_v2
        decay_params:
            model_dim: 512
            warmup_steps: 4000
        decay_step_duration: 8
        start_decay_steps: 0
        gradients_accum: 1
        clip_gradients: Null

dis_training_step: 2
dis_step: 10000
coeff_increasing_interval: 100000
lambda_E: 0.01

src_sharing_embedding_region_size: 488

src_domain_embedding_region_size: 
        - 8
        - 8
        - 8
tgt_embedding_size: 512

hidden_size: 600

printing_freq: 200
save_freq: 2000
eval_freq: 2000
summary_freq: 200

max_to_keep : 2

sample_buffer_size : 22000000

latent_variable_size : 1000

latent_variable_size_for_output : 300

use_kl_annealing : True

decoder_input_word_dropout_rate : 0.1

beam_width_for_generation : 5

batch_size_for_generation : 10

max_length_for_generation : 20

min_length_for_generation : 1
